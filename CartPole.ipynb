{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJyn98Qz9wtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pOCImbAAyb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7ss8GMY9fyD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15adf230-77d1-4e42-80b5-162721243e41"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrTLZxIABT3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('temp.npy', 'rb') as f:\n",
        "    screen = np.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHW4CN8XELaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CartPoleDataset(Dataset):\n",
        "  def __init__(self, data, transform=None):\n",
        "    super().__init__()\n",
        "    self.data = data\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    image, label = self.data[index]\n",
        "    image = np.asarray(image)\n",
        "    if self.transform != None:\n",
        "      image = self.transform(image)\n",
        "\n",
        "    return image, label"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_alVroqZaSHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "f9dfd5c8-74c1-4164-a1da-9da137cf325e"
      },
      "source": [
        "img = Image.fromarray(screen, 'RGB')\n",
        "img = img.resize((64, 64), Image.ANTIALIAS)\n",
        "\n",
        "data = [[img, 1]]\n",
        "\n",
        "img"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAABp0lEQVR4nO2YsU7DQAyGfydOo5Zn6NAiXoQyt0MnxFMw8EY8RsvCjEBiQ6KVKnWLgAE1FO7ODGFgKgOxrEj+1pPs++7su+RIRNBlMusJ/BcXsMYFrHEBa1zAGhewxgWscQFrXMAaF7DGBaxxAWtc4C9Ekmp88pe5Q4SP99f1vSTFTdASaCqnftlubq+JlJIA6j0gwuURSDFL55vYj9E/0Cx/ANoCIhGqLawt8LV7y3t9AFC7bbwHrHGBwxDpVX+DrkDc1xkXqinUBAQA4n6X9wYABB09hYigNvWGzjcxK8WNMSKllFISCSEAwqyyWFoCXBQA+v1BKktmrSwAKLX9uyQiRHSzXDyv1qN8Gz93m/zkeDw6nZw1Q+2mg+gwnc4AXMwvz+dXAKbTmVIirqqq3RWJMeZ5XhTMzE/rx37ZY+ai4KqqmqF203Fd1+1GbGYZQgBw97AgAhGHEOq6VhEYDoftRmyIMTYOAIAQY1RKpNXEy+VytVplWQYgpTQejyeTiSg0cecfthQvst9LQ0StV/9P5K7vQOe/hVzAGhewxgWscQFrOi/wDXtuU43pfnsZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F3561D4EC88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JapQ6NsiXH9R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 1\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "cart_pole_dataset = CartPoleDataset(data, transform=transform)\n",
        "data_loader = DataLoader(dataset=cart_pole_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHexd0eYYi90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "6fd8851f-29d0-4046-c26c-368d661b25f6"
      },
      "source": [
        "def flaotTensorToImage(x, mean=0, std=1):\n",
        "  x = np.transpose(x.numpy(), (1, 2, 0))\n",
        "  x = (x*std+ mean)*255\n",
        "  x = x.astype(np.uint8)    \n",
        "  return x\n",
        "\n",
        "for batch_idx, (data, action) in enumerate(data_loader):\n",
        "  data = data\n",
        "\n",
        "data = data[0]\n",
        "data = flaotTensorToImage(data)\n",
        "img = Image.fromarray(data, 'RGB')\n",
        "img"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py:56: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  img = torch.from_numpy(pic.transpose((2, 0, 1)))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAABp0lEQVR4nO2YsU7DQAyGfydOo5Zn6NAiXoQyt0MnxFMw8EY8RsvCjEBiQ6KVKnWLgAE1FO7ODGFgKgOxrEj+1pPs++7su+RIRNBlMusJ/BcXsMYFrHEBa1zAGhewxgWscQFrXMAaF7DGBaxxAWtc4C9Ekmp88pe5Q4SP99f1vSTFTdASaCqnftlubq+JlJIA6j0gwuURSDFL55vYj9E/0Cx/ANoCIhGqLawt8LV7y3t9AFC7bbwHrHGBwxDpVX+DrkDc1xkXqinUBAQA4n6X9wYABB09hYigNvWGzjcxK8WNMSKllFISCSEAwqyyWFoCXBQA+v1BKktmrSwAKLX9uyQiRHSzXDyv1qN8Gz93m/zkeDw6nZw1Q+2mg+gwnc4AXMwvz+dXAKbTmVIirqqq3RWJMeZ5XhTMzE/rx37ZY+ai4KqqmqF203Fd1+1GbGYZQgBw97AgAhGHEOq6VhEYDoftRmyIMTYOAIAQY1RKpNXEy+VytVplWQYgpTQejyeTiSg0cecfthQvst9LQ0StV/9P5K7vQOe/hVzAGhewxgWscQFrOi/wDXtuU43pfnsZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F3561D475C0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue8_EVt5Eeh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self, h, w, outputs):\n",
        "    super(DQN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "    self.bn2 = nn.BatchNorm2d(32)\n",
        "    self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "    self.bn3 = nn.BatchNorm2d(32)\n",
        "\n",
        "    def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
        "        return (size - (kernel_size - 1) - 1) // stride  + 1\n",
        "\n",
        "    convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
        "    convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
        "    linear_input_size = convw * convh * 32\n",
        "    self.head = nn.Linear(linear_input_size, outputs)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.bn1(self.conv1(x)))\n",
        "    x = F.relu(self.bn2(self.conv2(x)))\n",
        "    x = F.relu(self.bn3(self.conv3(x)))\n",
        "    return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyABGMM9NCzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dqn = DQN(64, 64, 2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oazcU6Y8NUuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b445e672-3c24-4a69-be3e-35dd04c89078"
      },
      "source": [
        "for batch_idx, (data, action) in enumerate(data_loader):\n",
        "  data = data\n",
        "\n",
        "dqn(data)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0456, -0.0600]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}